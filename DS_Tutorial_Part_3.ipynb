{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinelK/IMBIZO2022_DS_Tutorial/blob/main/DS_Tutorial_Part_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYnZpaluodGM"
      },
      "source": [
        "## Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQWtwVarOAha"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import h5py\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.linalg import inv\n",
        "from jax import random, vmap\n",
        "from jax.experimental import optimizers\n",
        "from jax.config import config\n",
        "from jax.nn import relu\n",
        "from jax.random import poisson as jpoisson\n",
        "#config.update(\"jax_debug_nans\", True) # Useful for finding numerical errors\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # original CPU-backed NumPy\n",
        "import os\n",
        "import sys\n",
        "from importlib import reload\n",
        "from tqdm import tqdm\n",
        "from matplotlib import rcParams\n",
        "import gc\n",
        "\n",
        "rcParams.update({'font.size': 18})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-YGOft2OzXh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! git clone -b SA_tutorial https://github.com/NinelK/jax-lfads.git\n",
        "# ! rm -r /content/jax-lfads/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yb32yVUOY9i"
      },
      "outputs": [],
      "source": [
        "# You must change this to the location of the computation-thru-dynamics directory.\n",
        "HOME_DIR = '/content'  # if running locally -- change this to YOUR path to the folder containing this notebook\n",
        "\n",
        "# import jax implementstion of lfads from lfads authors \n",
        "sys.path.append(os.path.join(HOME_DIR,'jax-lfads'))\n",
        "import lfads_tutorial.lfads as lfads\n",
        "import lfads_tutorial.utils as utils\n",
        "from lfads_tutorial.optimize import optimize_lfads, get_kl_warmup_fun\n",
        "import fixed_point_finder.fixed_points as fp_optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SmtCo8f30dsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJfOqmLlQaLO"
      },
      "outputs": [],
      "source": [
        "# Make directories\n",
        "lfads_dir = os.path.join(HOME_DIR,'lfads')       # where to save lfads data and parameters to\n",
        "output_dir = os.path.join(lfads_dir, 'output/')\n",
        "figure_dir = os.path.join(lfads_dir, os.path.join(output_dir, 'figures/'))\n",
        "if not os.path.exists(lfads_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(figure_dir):\n",
        "    os.makedirs(figure_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMrgtzJfmhSY"
      },
      "source": [
        "# Part 3: Recurrent neural networks (RNNs) as dynamical systems"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla RNN\n"
      ],
      "metadata": {
        "id": "i6x2c-sj3rXi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSqJHPB9fs7o"
      },
      "source": [
        "Let us first consider the simplest, yet most important example of a non-linear discrete-time latent dynamical system: a **recurrent neural network**. It is a go-to tool for modelling sequences in deep learning.\n",
        "\n",
        "Latent dynamics: $$h_{t} = \\sigma (V h_{t-1} + U x_t + b_h)$$\n",
        "Emission model: $$o_t = W h_t + b_o $$\n",
        "\n",
        "\n",
        "![RNN scheme (Wiki)](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2560px-Recurrent_neural_network_unfold.svg.png)\n",
        "\n",
        "\n",
        "RNNs are used in neuroscience for various purposes: \n",
        "* as a normative model trained to perform a certain task (a task-trained RNN);\n",
        "* as an ML blackbox tool for behavioral seq2seq decoding (inputs: neural spike trains, outputs: behavior).\n",
        "* as a model of neural population activity.\n",
        "\n",
        "We will focus on population models in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBCUUtvMoZ_n"
      },
      "source": [
        "#### Exercise 1: Implement a Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjJJrBIWpJOo"
      },
      "outputs": [],
      "source": [
        "def vanilla_rnn_params(key, n, u, ifactor=1.0, hfactor=1., hscale=0.0):\n",
        "  \"\"\"Initialize Vanilla RNN parameters for LFADS\n",
        "\n",
        "  Arguments:\n",
        "    key: random.PRNGKey for random bits\n",
        "    n: hidden state size\n",
        "    u: input size\n",
        "    ifactor: scaling factor for input weights\n",
        "    hfactor: scaling factor for hidden -> hidden weights\n",
        "    hscale: scale on h0 initial condition\n",
        "\n",
        "  Returns:\n",
        "    a dictionary of parameters\n",
        "  \"\"\"\n",
        "  key, skeys = utils.keygen(key, 3)\n",
        "  ifactor = ifactor / jnp.sqrt(u)\n",
        "  hfactor = hfactor / jnp.sqrt(n)\n",
        "\n",
        "  h0 = random.normal(next(skeys), (n,)) * hscale\n",
        "  wA = random.normal(next(skeys), (n,n)) * hfactor\n",
        "  wB = random.normal(next(skeys), (n,u)) * ifactor\n",
        "  wb = jnp.zeros((n,))\n",
        "\n",
        "  return {'h0' : h0,\n",
        "          'wA' : wA,\n",
        "          'wB' : wB,\n",
        "          'wb' : wb}\n",
        "\n",
        "\n",
        "def vanilla_rnn(params, h, x, nonlin=jnp.tanh):\n",
        "  \"\"\"Implement the Vanilla RNN equations.\n",
        "\n",
        "  Arguments:\n",
        "    params: dictionary of RNN parameters\n",
        "    h: (np array) of hidden state components\n",
        "    x: (np array) of input components\n",
        "    nonlin: (function) a nonlinearity\n",
        "\n",
        "  Returns:\n",
        "    np array of hidden state after RNN update\"\"\"\n",
        "  # using the weights specified in vanilla_rnn_params, e.g. params['wA']\n",
        "  # use jnp.dot(...,...) for multiplying matrices using JAX\n",
        "  # h_new = ... # FILL IN\n",
        "  return h_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN-based generative models of neuronal population dynamics"
      ],
      "metadata": {
        "id": "orc-zL2jNDdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, fitting an RNN to data allows one to speculate about the *computation* that the neural population is performing.\n",
        "However, there are multiple options for describing neuronal populations with RNNs, which can be divided into 2 large groups [3]: \n",
        "1. **Fully-observed models:** In these models, RNN units (components of vector $\\mathbf{h}$) would have 1-to-1 correspondence to single neural firing rates, while outputs $\\mathbf{o}$ can correspond to some behavioral outputs (e.g. hand position in space).\n",
        "2. **Latent dynamics models:** in these models neural firing rates would correspond to the outputs $\\mathbf{o}$, while $\\mathbf{h}$ (latent state) would summarize what a neuronal population as a whole is doing.\n",
        "\n",
        "![two model types](https://drive.google.com/uc?export=view&id=16G4HWvarW8rtNlBSlnCvCIcJq78Pr7Lu)\n",
        "\n",
        "In this tutorial, we will focus on the second class of models: the models of latent dynamics. \n",
        "\n",
        "These models are useful for understanding modern single-unit recordings, which can have 100-1000-10000 of recorded neurons simultaneously. We can't visualize or analyse such high-dimensional data!\n",
        "At the same time, it was shown that neural population activity often has a low-dimensional structure: a low number of latent dynamical factors can explain a large fraction of neural variability. This finding is called a 'manifold hypothesis', and was proposed in [Vyas et al. 2020](#references).\n",
        "\n"
      ],
      "metadata": {
        "id": "Jl64ZyQaOmEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward problem: generating data from a latent dynamics model"
      ],
      "metadata": {
        "id": "9HxorzXgSd0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 2: Generate some spike data from a VRNN\n",
        "\n",
        "Suppose a neural population with $N$ neurons follows a simple dynamics. Let's imagine that it oscillates. We can describe this dynamics with 2 latent factors, similarly to a pendulum in Tutorial Part 1.\n",
        "\n",
        "$${\\begin{bmatrix}\n",
        "    \\dot h_1 \\\\\n",
        "    \\dot h_2\n",
        "\\end{bmatrix}} = \\begin{bmatrix}\n",
        "    0 & 1 \\\\\n",
        "    -1 & 0\n",
        "\\end{bmatrix} \\begin{bmatrix}\n",
        "    h_1 \\\\\n",
        "    h_2\n",
        "\\end{bmatrix} \\tag{2a}$$\n",
        "\n",
        "Now, suppose log-firing rate of each neuron in a population is a sum of these two factors with some arbitrarily chosen coefficients:\n",
        "\n",
        "$${\\begin{bmatrix}\n",
        "    \\log r_1 \\\\\n",
        "    \\log r_2 \\\\\n",
        "    \\ldots \\\\\n",
        "    \\log r_N\n",
        "\\end{bmatrix}} = {\\begin{bmatrix}\n",
        "    o_1 \\\\\n",
        "    o_2 \\\\\n",
        "    \\ldots \\\\\n",
        "    o_N\n",
        "\\end{bmatrix}} = \\underbrace{B}_{N \\times 2} \\begin{bmatrix}\n",
        "    h_1 \\\\\n",
        "    h_2\n",
        "\\end{bmatrix} + {\\begin{bmatrix}\n",
        "    b_1 \\\\\n",
        "    b_2 \\\\\n",
        "    \\ldots \\\\\n",
        "    b_N\n",
        "\\end{bmatrix}} \\tag{2b}$$\n",
        "\n",
        "$$\\mathrm{spikes}_i \\sim \\mathrm{Poisson}(r_i) \\tag{2c}$$\n",
        "\n",
        "Let us implement this model and run in a *forward* mode to generate some spikes:"
      ],
      "metadata": {
        "id": "GyR_q1MLWncL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emission_model(params, h, key):\n",
        "  key, skeys = utils.keygen(key, 1)\n",
        "  o = jnp.dot(params['wW'],h) + params['wbo']\n",
        "  s = jpoisson(next(skeys), jnp.exp(o))\n",
        "  return s\n",
        "\n",
        "def generative_model(params, h, x, skeys):\n",
        "  # h = ... # FILL IN hidden state update\n",
        "  # s = ... # FILL IN spike output\n",
        "  return h, s\n",
        "\n",
        "n,N = 2, 100\n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "w = .2\n",
        "params = {'wA' : jnp.array([[np.sqrt(1.-w**2),w],[-w,np.sqrt(1.-w**2)]]),\n",
        "          'wB' : jnp.zeros((2,2)),\n",
        "          'wb' : jnp.zeros((n,)),\n",
        "          'wW' : random.normal(key, (N,n)),\n",
        "          'wbo': jnp.zeros((N,))}\n",
        "T = 50\n",
        "num_trials = 200\n",
        "synth_spikes = []\n",
        "ic_angle = []\n",
        "for _ in tqdm(range(num_trials)):\n",
        "  a = np.random.uniform() *2 * jnp.pi - jnp.pi\n",
        "  h = jnp.array([np.sin(a),np.cos(a)]).flatten()\n",
        "  spikes_this_trial = []\n",
        "  for t in range(T):\n",
        "    key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "    h, s = generative_model(params, h, jnp.zeros((n,)), key)\n",
        "    spikes_this_trial.append(s)\n",
        "  synth_spikes.append(spikes_this_trial)\n",
        "  ic_angle.append(a)"
      ],
      "metadata": {
        "id": "-QPVsrQ82jJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1oUSJD_LtKm"
      },
      "source": [
        "## Inverse problem: inferring dynamics & initial conditions using LFADS\n",
        "\n",
        "We have generated some spike trains, for which we know the uderlying DS (the initial conditions and true underlying dynamics). In real experiments, the true dynamics and initial conditions are NOT known, but it would be useful to find the *most likely* DS that could have produced these spikes. Once we have this DS, we can analyse it and describe what this DS is doing like in Tutorial Parts 1-2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This inverse problem is not an easy machine learning problem. It involves many concepts, such as Bayesian statistics, that you will study later during the school. Here, we won't go into details and will use one of the existing solutions: an LFADS framework.\n",
        "\n",
        "**LFADS** stands for Latent Factor Analysis via Dynamical Systems [Pandarinath et al. 2018](#references). It takes in a sequence of spikes in a format: \\[trials x time x neurons\\]. LFADS is a auto-encoder: it **encodes** spike sequences into initial conditions, and then **decodes** firing rates from these initial conditions using a dynamical generative model:  \n",
        "\n",
        "![autonomous_LFADS](https://drive.google.com/uc?export=view&id=1lz3sL5NAbWLaQl0yZ8nLcyD4JgHCwGns)\n",
        "\n",
        "As an auto-encoder, LFADS implements an **information bottleneck**: it gets rid of all unimportant variations in the data, but keeps the meaningful information in initial conditions. \n",
        "\n",
        "LFADS tells us how *likely* it is that the recorded spikes were produced by a dynamical model (2a-2c) and initial conditions $h_0$ ($g_0$ in LFADS paper). So, the loss function we are going to optimize is:\n",
        "\n",
        "$$\\mathcal{L}(\\Theta) = \\underbrace{- \\left < \\sum_{t=0}^T \\log p_\\Theta (\\mathrm{\\mathbf{spikes}}_t | \\mathbf{r}_t) \\right >_{\\mathbf{g_0}}}_{LL} + \\mathrm{penalties} \\tag{3}$$\n",
        "where $\\mathrm{\\mathbf{spikes}}_t$ and $\\mathbf{r}_t$ are vectors of size $N$, where $N$ is a number of neurons. The likelihood $p_\\Theta (\\mathrm{\\mathbf{spikes}}_t | \\mathbf{r}_t)$ corresponds to the Poisson **likelihood** (a term from Baysian statistics that you'll learn soon): it tells how *likely* it is that neural population produced a certain number of spikes ($\\mathrm{\\mathbf{spikes}}_t$) in a time bin $t$, *given* the firing rate $\\mathbf{r}_t$. $\\Theta$ here corresponds to all 'trainable' parameters in an LFADS model: the parameters we are going to optimize to minimize the loss $\\mathcal{L}(\\Theta)$."
      ],
      "metadata": {
        "id": "dtedcBoWZBjZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ_r1bIJHPA7"
      },
      "source": [
        "**How do we train it?**\n",
        "\n",
        "Similarly to the multi-layer networks that you have seen in a previous tutorial, RNNs can be trained through backpropagation. Although, instead of propagating gradients through layers, here we would propagate gradients throught **time**. To see how this can be done, we need to unfold timesteps into a sequence (right plot above). In these unfolded scheme, each column is a 2-layer perceptron. The new thing is a transition from the previous time step to the next. This is called backpropagation through time (BPTT)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are so many parameters $\\Theta$ in this model... is it a problem?**\n",
        "\n",
        "LFADS is a statistical model. We will use some data ('training set') to train the model: optimize its parameters $\\Theta$ such that the loss $\\mathcal{L}$ (3) is minimal. There is always a risk that such model memorizes the training examples and can not make predictions on some new, unseen examples ('evaluation set').\n",
        "This is called **overfitting**. To prevent overfitting, we are adding penalties to the loss (3), which force the model to be as simple as possible.\n",
        "\n",
        "To formally check whether the model overfits, we need to provide the model with new, unseen examples and check the log-likelihood (LL in (3)) on the evaluation set. Let us then split all available data into train and evaluation sets and leave the evaluation set aside for now:"
      ],
      "metadata": {
        "id": "JgmB3-9qM5tA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBEJMxBylGo5"
      },
      "outputs": [],
      "source": [
        "# convert synthetic data to numpy arrays\n",
        "data_bxtxn = np.array(synth_spikes)\n",
        "ic_angle = np.array(ic_angle).flatten()\n",
        "\n",
        "train_fraction = 0.9     # Train with 90% of the synthetic data, evaluate on 10%\n",
        "nexamples, ntimesteps, data_dim = data_bxtxn.shape # get the shape of all data\n",
        "\n",
        "train_data, eval_data = utils.split_data(data_bxtxn,\n",
        "                                         train_fraction=train_fraction) # split\n",
        "eval_data_offset = int(train_fraction * data_bxtxn.shape[0])\n",
        "\n",
        "print(f'First {eval_data_offset} out of {nexamples} trials will be used in training.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set LFADS Parameters (you can ignore them)"
      ],
      "metadata": {
        "id": "PI-CTJ0cRWKs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPH3GH-Dnkmd"
      },
      "outputs": [],
      "source": [
        "# LFADS Hyper parameters\n",
        "data_dim = train_data.shape[2]  # input to lfads should have dimensions:\n",
        "ntimesteps = train_data.shape[1] #   (batch_size x ntimesteps x data_dim)\n",
        "batch_size = 200      # batch size during optimization\n",
        "\n",
        "# LFADS architecture - The size of the numbers is rather arbitrary, \n",
        "# but relatively small because we know the integrator RNN isn't too high \n",
        "# dimensional in its activity.\n",
        "enc_dim = 2         # encoder dim\n",
        "con_dim = 0          # controller dim\n",
        "ii_dim = 0           # inferred input dim\n",
        "gen_dim = 2         # generator dim, should be large enough\n",
        "factors_dim = gen_dim      # factors dim, should be large enough to capture most variance of dynamics\n",
        "\n",
        "# Numerical stability\n",
        "var_min = 0.001 # Minimal variance any gaussian can become.\n",
        "\n",
        "# Optimization HPs that percolates into model\n",
        "l2reg = 0.00002\n",
        "\n",
        "# Initial state prior parameters\n",
        "# the mean is set to zero in the code\n",
        "ic_prior_var = 0.1 # this is $\\sigma^2_p$ in above paragraph\n",
        "\n",
        "# Inferred input autoregressive prior parameters\n",
        "ar_mean = 0.0                 # process mean\n",
        "ar_autocorrelation_tau = 1.0  # seconds, how correlated each time point is, related to $\\phi$ above.\n",
        "ar_noise_variance = 0.1       # noise variance\n",
        "\n",
        "lfads_hps = {'data_dim' : data_dim, 'ntimesteps' : ntimesteps,\n",
        "             'enc_dim' : enc_dim, 'con_dim' : con_dim, 'var_min' : var_min,\n",
        "             'ic_prior_var' : ic_prior_var, 'ar_mean' : ar_mean,\n",
        "             'ar_autocorrelation_tau' : ar_autocorrelation_tau,\n",
        "             'ar_noise_variance' : ar_noise_variance,\n",
        "             'ii_dim' : ii_dim, 'gen_dim' : gen_dim,\n",
        "             'factors_dim' : factors_dim,\n",
        "             'l2reg' : l2reg,\n",
        "             'batch_size' : batch_size}\n",
        "\n",
        "num_batches = 3000         # how many batches do we train\n",
        "print_every = 100            # give information every so often\n",
        "\n",
        "# Learning rate HPs\n",
        "step_size = 0.01            # initial learning rate\n",
        "decay_factor = 0.9998       # learning rate decay param\n",
        "decay_steps = 1             # learning rate decay param\n",
        "\n",
        "# Regularization HPs\n",
        "keep_rate = 0.95             # dropout keep rate during training\n",
        "\n",
        "# Numerical stability HPs\n",
        "max_grad_norm = 10.0        # gradient clipping above this value\n",
        "\n",
        "# The fact that the start and end values are required to be floats is something I need to fix.\n",
        "kl_warmup_start = 0.0 # batch number to start KL warm-up, explicitly float\n",
        "kl_warmup_end = 1000.0  # batch number to be finished with KL warm-up, explicitly float\n",
        "kl_min = .1 # The minimum KL value, non-zero to make sure KL doesn't grow crazy before kicking in.\n",
        "kl_max = 1.\n",
        "\n",
        "lfads_opt_hps = {'num_batches' : num_batches, 'step_size' : step_size,\n",
        "                 'decay_steps' : decay_steps, 'decay_factor' : decay_factor,\n",
        "                 'kl_min' : kl_min, 'kl_max' : kl_max, 'kl_warmup_start' : kl_warmup_start,\n",
        "                 'kl_warmup_end' : kl_warmup_end, 'keep_rate' : keep_rate,\n",
        "                 'max_grad_norm' : max_grad_norm, 'print_every' : print_every,\n",
        "                 'adam_b1' : 0.9, 'adam_b2' : 0.999, 'adam_eps' : 1e-4}\n",
        "\n",
        "class hashabledict(dict):\n",
        "    def __hash__(self):\n",
        "        return hash(tuple(sorted(self.items())))\n",
        "\n",
        "lfads_hps = hashabledict(lfads_hps)\n",
        "lfads_opt_hps = hashabledict(lfads_opt_hps)\n",
        "\n",
        "assert num_batches >= print_every and num_batches % print_every == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize LFADS"
      ],
      "metadata": {
        "id": "SlfuSm79Ra1A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqWX6Kl4oGCq"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters for LFADS\n",
        "from functools import partial\n",
        "\n",
        "# setting up the generator model\n",
        "rnn_type = 'vanilla_rnn'\n",
        "nonlin = lambda x: jnp.tanh(x) # specify a nonlinearity\n",
        "model = partial(vanilla_rnn, **{'nonlin': nonlin})\n",
        "model_params = vanilla_rnn_params\n",
        "  \n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "init_params = lfads.lfads_params(key, lfads_hps, gen_rnn_params=model_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train LFADS"
      ],
      "metadata": {
        "id": "jSGpt0EbRdHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ruJdYukooPT"
      },
      "outputs": [],
      "source": [
        "# Takes 3 minutes to train on a GPU; first step takes ~13 sec.\n",
        "# roughly the same time on a CPU, because the model is small\n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "trained_params, opt_details = \\\n",
        "    optimize_lfads(key, init_params, lfads_hps, lfads_opt_hps,\\\n",
        "                   train_data, eval_data, gen=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhrKd0T_pFEC"
      },
      "outputs": [],
      "source": [
        "# saving model parameters of the trained model\n",
        "fname_uniquifier = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "network_fname = (f'trained_params_lfads_pend_{rnn_type}' + fname_uniquifier + '.npz')\n",
        "network_path = os.path.join(output_dir, network_fname)\n",
        "\n",
        "print(\"Saving parameters: \", network_path)\n",
        "np.savez(network_path, trained_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLo5QVPM76Eh"
      },
      "outputs": [],
      "source": [
        "# Plot the training details\n",
        "x = np.arange(0, num_batches, print_every)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(x, opt_details['tlosses']['nlog_p_xgz'], 'k',label='Training')\n",
        "plt.ylabel('Total loss')\n",
        "plt.plot(x, opt_details['elosses']['nlog_p_xgz'], 'C1',label='Evaluation')\n",
        "plt.xlabel('Training batch')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiEJTNiKQUOf"
      },
      "source": [
        "### Visualize latent space trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lX7eD0ozQLmp"
      },
      "outputs": [],
      "source": [
        "# Sample a bunch of example trajectories for eval trials\n",
        "\n",
        "bidx = 0 # trial number\n",
        "\n",
        "nexamples_to_save = 1\n",
        "for eidx in range(nexamples_to_save):\n",
        "    fkey = random.fold_in(key, eidx)\n",
        "    psa_example = eval_data[bidx,:,:].astype(jnp.float32)\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, \n",
        "                                                      fkey, psa_example, gen=model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXh3d4vBQWBC"
      },
      "outputs": [],
      "source": [
        "plt.plot(psa_dict['factor_t'])\n",
        "plt.title(f'Factors for trial {bidx}')\n",
        "plt.xlabel('time bins')\n",
        "plt.ylabel('factor values')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPgWndF3Qd7N"
      },
      "outputs": [],
      "source": [
        "# sample trajectories for many trials\n",
        "ics = np.empty((data_bxtxn.shape[0],gen_dim))\n",
        "gen_traj = np.empty((data_bxtxn.shape[0],ntimesteps,gen_dim))\n",
        "for i, psa_example in enumerate(data_bxtxn.astype(jnp.float32)):\n",
        "    psa_dict = lfads.posterior_sample_and_average_jit(trained_params, lfads_hps, \n",
        "                                                      fkey, psa_example, gen=model)\n",
        "    ics[i] = psa_dict['ic_mean']\n",
        "    gen_traj[i] = psa_dict['gen_t']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEK3_yJf3LKw"
      },
      "outputs": [],
      "source": [
        "for a,t in zip(ic_angle,gen_traj):\n",
        "  plt.plot(*t.T, c=plt.cm.rainbow(a / (2*np.pi) + 0.5),alpha=0.5)\n",
        "plt.xlabel('Factor 1')\n",
        "plt.ylabel('Factor 2');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixed point analysis\n",
        "\n",
        "We can not find fixed points analytically anymore, because of nonlinearity. Instead, we will start from many initial conditions and check where the trajectories get 'stuck'. Trained rarely have 'true' fixed points. More often, they have 'slow points': the points where the state **almost** does not change. "
      ],
      "metadata": {
        "id": "QoCQp4pRWTrl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Think!**\n",
        "\n",
        "Will we approximate all types of fixed points with the 'slow points' approximation?"
      ],
      "metadata": {
        "id": "MgDZDckcjdQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3: Linearizing around the fixed point: calculating the Jacobian"
      ],
      "metadata": {
        "id": "BtiOBMWALQOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect() # free up some RAM"
      ],
      "metadata": {
        "id": "jZXOEIemX0dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are some preliminaries. \n",
        "x_star = np.zeros(ii_dim)  # the input is zero in this example.\n",
        "\n",
        "# Make a one parameter function of thie hidden state, useful for jacobians.\n",
        "key = random.PRNGKey(np.random.randint(0, utils.MAX_SEED_INT))\n",
        "rnn_fun = lambda h : model(trained_params['gen'], h, x_star)\n",
        "batch_rnn_fun = vmap(rnn_fun, in_axes=(0,))\n"
      ],
      "metadata": {
        "id": "G2wTGcuoW57E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create some functions that define the fixed point loss\n",
        "which is just the squared error of a point for a discrete time system such as a VRNN or GRU: $(h_{n+1} - h_n)^2$"
      ],
      "metadata": {
        "id": "86n9OXzCXuSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp_loss_fun = fp_optimize.get_fp_loss_fun(rnn_fun)\n",
        "total_fp_loss_fun = fp_optimize.get_total_fp_loss_fun(rnn_fun)"
      ],
      "metadata": {
        "id": "P-soJ5-MXsld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to start the fixed point finder with some points, and it's always best to start with examples of where the state normally operates.\n"
      ],
      "metadata": {
        "id": "pY9w2VzWYRGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fp_candidates = jnp.array(gen_traj[::50,])                 # was batch x time x dim\n",
        "fp_candidates = jnp.reshape(fp_candidates, (-1, gen_dim))  # now (batch * time) x dim"
      ],
      "metadata": {
        "id": "QyuLmgv5YPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed point optimization hyperparameters\n",
        "fp_num_batches = 1000         # Total number of batches to train on.\n",
        "fp_batch_size = 100          # How many examples in each batch\n",
        "fp_step_size = 0.2          # initial learning rate\n",
        "fp_decay_factor = 0.9999     # decay the learning rate this much\n",
        "fp_decay_steps = 1           #\n",
        "fp_adam_b1 = 0.9             # Adam parameters\n",
        "fp_adam_b2 = 0.999\n",
        "fp_adam_eps = 1e-5\n",
        "fp_opt_print_every = 200   # Print training information during optimziation every so often\n",
        "\n",
        "# Fixed point finding thresholds and other HPs\n",
        "fp_noise_var = 0.0      # Gaussian noise added to fixed point candidates before optimization.\n",
        "fp_opt_stop_tol = 0.00001  # Stop optimizing when the average value of the batch is below this value.\n",
        "fp_tol = 0.00001        # Discard fps with squared speed larger than this value.\n",
        "fp_unique_tol = 0.025      # tolerance for determination of identical fixed points\n",
        "fp_outlier_tol = 1.0    # Anypoint whos closest fixed point is greater than tol is an outlier.\n"
      ],
      "metadata": {
        "id": "10Q2MotSZFTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reload(fp_optimize)\n",
        "\n",
        "fp_hps = {'num_batches' : fp_num_batches, 'step_size' : fp_step_size, \n",
        "            'decay_factor' : fp_decay_factor, 'decay_steps' : fp_decay_steps, \n",
        "            'adam_b1' : fp_adam_b1, 'adam_b2' : fp_adam_b2,\n",
        "            'adam_eps' : fp_adam_eps, 'noise_var' : fp_noise_var, \n",
        "            'fp_opt_stop_tol' : fp_opt_stop_tol, 'fp_tol' : fp_tol, \n",
        "            'unique_tol' : fp_unique_tol, 'outlier_tol' : fp_outlier_tol, \n",
        "            'opt_print_every' : fp_opt_print_every}\n",
        "\n",
        "fp_hps = hashabledict(fp_hps)\n",
        "\n",
        "fps, fp_losses, fp_idxs, fp_opt_details = \\\n",
        "  fp_optimize.find_fixed_points(rnn_fun, fp_candidates, fp_hps, do_print=True)\n",
        "if len(fp_idxs) > 0:\n",
        "    F_of_fps = batch_rnn_fun(fps)\n",
        "else:\n",
        "    F_of_fps = np.zeros([0,gen_dim])\n",
        "    \n",
        "fps = {'fps' : fps, 'candidates' : fp_candidates[fp_idxs],\n",
        "                'losses' : fp_losses, 'F_of_fps' : F_of_fps, \n",
        "                'opt_details' : fp_opt_details, 'hps' : fp_hps}\n",
        "\n",
        "fps\n"
      ],
      "metadata": {
        "id": "JdmPOAmTZUpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a,t in zip(ic_angle,gen_traj):\n",
        "  plt.plot(*t.T, c=plt.cm.rainbow(a / (2*np.pi) + 0.5),alpha=0.5)\n",
        "assert len(fps['fps']) == 1, 'That\\'s strange! The cell above found more than one point'\n",
        "plt.scatter(*fps['fps'][0],marker='x',c='k')\n",
        "plt.xlabel('Factor 1')\n",
        "plt.ylabel('Factor 2');"
      ],
      "metadata": {
        "id": "cxAAtlz0Bh09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see one fixed point in the center of rotation. Looks good so far!\n",
        "\n",
        "Now because we use JAX already, we can compute the Jacobian $J = \\partial(h_{n+1})/\\partial(h_{n})$ using built-in automatic differentiation:"
      ],
      "metadata": {
        "id": "X2aVn6TmMNfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jacs = fp_optimize.compute_jacobians(rnn_fun, fps['fps'])\n",
        "J = jacs[0] # still assuming just 1 fixed point\n",
        "print('Jacobian:\\n',J)"
      ],
      "metadata": {
        "id": "qTauv2SKLu8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our non-linear dynamical system is approximated as $h_{n+1} \\approx J h_n$. It is a linear system! \n",
        "\n",
        "Let us just the eigendecomposition for it:"
      ],
      "metadata": {
        "id": "4B_-bvCyXSDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate eigenvalues of J\n",
        "# FILL IN"
      ],
      "metadata": {
        "id": "msn91P_sT1L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is this system doing? Is the fixed point stable? What is the frequency of oscillations? "
      ],
      "metadata": {
        "id": "L90VMGj-XdjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra exercise 4: How is the Jacobian different from the learned matrix $A$? Why?"
      ],
      "metadata": {
        "id": "W809NyzrSjVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = trained_params['gen']['wA']\n",
        "A"
      ],
      "metadata": {
        "id": "O38dv48fPmAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Appendix**\n",
        "\n",
        "\n",
        "*More precisely, LFADS is a variational auto-encoder (VAE), which means that the bottleneck is achieved via probabilistic methods [0]*. The initial conditions are encoded in a distribution: a normal distribution $q_\\Theta(g_{0,j} | \\mathrm{spikes}) \\sim \\mathcal{N}(\\mu_j,\\sigma_j)$ (where $j$ is a dimension in the latent space). \n",
        "This distribution, calculated per-trial, is called an 'approximate posterior'. \n",
        "\n",
        "This 'generative' model tells us how *likely* it is that the recorded spikes were produced by a dynamical model (2a-2c) and initial conditions $h_0$ ($g_0$ in LFADS paper). So, the loss function we are going to optimize is:\n",
        "\n",
        "$$\\mathcal{L}(\\Theta) = \\underbrace{- \\left < \\sum_{t=0}^T \\log p_\\Theta (\\mathrm{\\mathbf{spikes}}_t | \\mathbf{r}_t) \\right >_{\\mathbf{g_0}}}_{LL} + \\mathrm{penalties} \\tag{3}$$\n",
        "where $\\mathrm{\\mathbf{spikes}}_t$ and $\\mathbf{r}_t$ are vectors of size $N$, where $N$ is a number of neurons. The likelihood $p_\\Theta (\\mathrm{\\mathbf{spikes}}_t | \\mathbf{r}_t)$ corresponds to the Poisson likelihood and tells how likely it is that neural population produced a certain number of spikes ($\\mathrm{\\mathbf{spikes}}_t$) in a time bin $t$, *given* the firing rate $\\mathbf{r}_t$. This likelihood is then averaged ($<.>$) across the samples from the posterior distribution of $\\mathbf{g}_0$. $\\Theta$ here corresponds to all 'trainable' parameters in an LFADS model: the parameters we are going to optimize to minimize the loss $\\mathcal{L}(\\Theta)$.\n",
        "\n",
        "LFADS is a statistical model. We will use some data ('training set') to train the model: optimize its parameters $\\Theta$ such that the loss $\\mathcal{L}$ (3) is minimal. There is always a risk that such model memorizes the training examples and can not make predictions on some new, unseen examples ('evaluation set').\n",
        "This is called **overfitting**. To prevent overfitting, we are adding penalties to the loss (3), which force the model to be as simple as possible.\n",
        "\n",
        "LFADS has 2 types of penalties:\n",
        "\n",
        "$$\\mathcal{L}(\\Theta) = \\underbrace{- \\left < \\sum_{t=0}^T \\log p_\\Theta (\\mathrm{\\mathbf{spikes}}_t | \\mathbf{r}_t) \\right >_{\\mathbf{g_0}}}_{LL} + \\left < D_{KL}(q_\\theta (\\mathbf{g}_0 | \\mathrm{spikes})~||~p(\\mathbf{g}_0))\\right >_{\\mathbf{g_0}} + L2(\\Theta) \\tag{4}$$\n",
        "\n",
        "🤯 **If you don't understand these penalties now -- it is ok! It will all makes sense ones you learned Bayesian statistics in the following weeks. For now, assume that penalties keep the model simple.**\n",
        "\n",
        "The KL-divergence term $D_{KL}$ keeps the initial conditions as uninformative as possible (i.e. keeps the approximate posterior as close to the prior as possible). It limits the amount of 'discriminative information' that can pass through the bottleneck and allows the model to discriminate trials from each other, preventing the model from learning trial identities as a lookup table. L2 term tries to keep all the trainable weights as small as possible."
      ],
      "metadata": {
        "id": "bIatiPB2wWJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project:** Modelling dynamics based on REAL data\n",
        "\n",
        "The next notebook is doing the same things as this one, but with REAL neural recordings. \n",
        "\n",
        "[Go to the next notebook](https://colab.research.google.com/drive/1CM6GLsDdUpaXcAnjjiLQGtUJZe5GH7MM?usp=sharing)"
      ],
      "metadata": {
        "id": "jE2TTKmgzwIP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFRVmFtnnm36"
      },
      "source": [
        "# References\n",
        "<a name=\"references\"></a>\n",
        "0. [Computation through dynamics tutorial](https://github.com/google-research/computation-thru-dynamics) from David Sussillo, which contains an implementation of LFADS that was adapted to this tutorial + extra material (in depth LFADS tutorial, inferring unobserved inputs, FORCE training)\n",
        "\n",
        "1. *Manifold hypothesis:* Gallego, Juan A., et al. [Neural manifolds for the control of movement.](https://pubmed.ncbi.nlm.nih.gov/28595054/) Neuron 94.5 (2017): 978-984.\n",
        "\n",
        "2. *A review of a population dynamics models:* Vyas, Saurabh, et al. \"Computation through neural population dynamics.\" Annual Review of Neuroscience 43 (2020): 249. [Not paywalled pdf.](https://web.stanford.edu/~mgolub/publications/2020-Vyas-ARN.pdf)\n",
        "\n",
        "3. *Our review on fully observed vs latent dynamical models*: Hurwitz, Cole, et al. [Building population models for large-scale neural recordings: Opportunities and pitfalls.](https://arxiv.org/pdf/2102.01807.pdf) Current Opinion in Neurobiology 70 (2021): 64-73.\n",
        "\n",
        "4. *The LFADS paper:* Pandarinath, Chethan, et al. [Inferring single-trial neural population dynamics using sequential auto-encoders.](https://www.nature.com/articles/s41592-018-0109-9) Nature methods 15.10 (2018): 805-815.\n",
        "  \n",
        "## Interesting recent papers for further reading\n",
        "\n",
        "If you want to know what's going on in the field right now, you can check the following papers (a very biased selection from Nina ;) ):\n",
        "\n",
        "1. Maheswaranathan, Niru, et al. [Universality and individuality in neural dynamics across large populations of recurrent networks.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7416639/) Advances in neural information processing systems 32 (2019).\n",
        "\n",
        "2. Duncker, Lea, et al. [Organizing recurrent network dynamics by task-computation to enable continual learning.](https://proceedings.neurips.cc/paper/2020/file/a576eafbce762079f7d1f77fca1c5cc2-Paper.pdf) Advances in Neural Information Processing Systems 33 (2020). \n",
        "\n",
        "3. Smith, Jimmy, Scott Linderman, and David Sussillo. [Reverse engineering recurrent neural networks with jacobian switching linear dynamical systems.](https://arxiv.org/pdf/2111.01256.pdf) Advances in Neural Information Processing Systems 34 (2021): 16700-16713.\n",
        "\n",
        "4. Hurwitz, Cole, et al. [Targeted neural dynamical modeling.](https://arxiv.org/pdf/2110.14853.pdf) Advances in Neural Information Processing Systems 34 (2021): 29379-29392.\n",
        "\n",
        "5. Schimel, Marine, Ta-Chu Kao, Kristopher T. Jensen, and Guillaume Hennequin. [iLQR-VAE: control-based learning of input-driven dynamics with applications to neural data.](https://www.biorxiv.org/content/10.1101/2021.10.07.463540v1.full.pdf) bioRxiv (2021). "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PI-CTJ0cRWKs",
        "jE2TTKmgzwIP"
      ],
      "name": "DS Tutorial Part 3",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNK1TNPwRLR/jZ9i1oZw6TU",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}